# Kookree Realâ€‘Time Image Classification Pipeline

A stepâ€‘byâ€‘step guide to spin up a full realâ€‘time imageâ€‘classification stack using PyTorchÂ (ONNX), Docker, gRPC, and Redpanda.

---

## ğŸ“¦ StepÂ 1Â â€“Â Clone & Install Host Dependencies

```bash
git clone https://github.com/YOUR_USER/kookree-pipeline.git
cd kookree-pipeline
# Install Python deps for producer / consumer & load test
pip install -r requirements.txt
```

---

## ğŸ³ StepÂ 2Â â€“Â Build and Start Core Services

```bash
# Build inference image & launch everything
docker compose build           # multiâ€‘stage build
docker compose up -d                             # inference, redpanda, prometheus, grafana
```

Services started:

| Service           | Purpose                                    | Ports             |
| ----------------- | ------------------------------------------ | ----------------- |
| inference_service | gRPCÂ 50051 Â· /healthzÂ 8080 Â· /metricsÂ 8000 | 50051,Â 8080,Â 8000 |
| redpanda          | Kafkaâ€‘compatible broker                    | 9092,Â 9644        |
| prometheus        | Metrics storage                            | 9090              |
| grafana           | Dashboard UI                               | 3000              |

Verify readiness:

```bash
curl http://localhost:8080/healthz   # â†’ OK
```

---

## ğŸ¥ StepÂ 3Â â€“Â Start Streaming Simulator

Launch producer (webcam 0, 15â€¯FPS) **and** consumer in one command:

```bash
python streaming_simulator/run_streaming.py \
    --source 0 \
    --fps 15 \
    --bootstrap localhost:9092 \
    --grpc localhost:50051 \
    --window 30
```

After 30â€¯s youâ€™ll see an FPS / latency summary in the terminal and in `logs/`.

---

## ğŸ§ª StepÂ 4Â â€“Â Loadâ€‘test the gRPC Endpoint (optional)

```bash
python tests/load_test_grpc.py \
    --grpc localhost:50051 \
    --requests 200 \
    --concurrency 20
```

Outputs total throughput, avg latency, and sample label.

---

## ğŸ“ˆ Monitoring (optional)

### Prometheus

- Inference metrics: <http://localhost:8000/metrics>
- Consumer metrics: <http://localhost:9100/metrics>

Prometheus UI: <http://localhost:9090>

### Grafana Dashboard

1. Open <http://localhost:3000> (admin / admin)
2. **DashboardsÂ â†’Â ImportÂ â†’Â Upload JSON**
3. Select `monitoring/grafana/dashboards/kookree_dashboard.json`
4. Click **Import** to visualise latency, FPS, errors, etc.

---

## ğŸ“‚ Log Files

```
logs/
â”œâ”€â”€ producer/YYYY-MM-DD_HH-MM-SS.log
â”œâ”€â”€ consumer/YYYY-MM-DD_HH-MM-SS.log
â””â”€â”€ inference_service.log
```

Each run creates fresh timestamped logs for traceability.

---

## ğŸ”„ Rebuild Tips

Enable caching of Torch download:

```dockerfile
RUN --mount=type=cache,target=/root/.cache \
    python inference_service/model/export_to_onnx.py
```

Reâ€‘build:

```bash
docker compose build --no-cache   # if needed
```

---

## â“ Troubleshooting

| Symptom               | Fix                                         |
| --------------------- | ------------------------------------------- |
| Redpanda not ready    | `docker compose logs redpanda`              |
| Camera not accessible | Run `producer.py` on host, not in container |
| gRPC errors           | Check `inference_service.log`               |

---
